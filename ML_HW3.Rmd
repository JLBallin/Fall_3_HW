---
title: "ML HW2"
output: html_document
date: "2023-11-26"
---

```{r}
library(vcdExtra)
library(car)
library(dplyr)
library(DescTools)
library(mgcv)
library(visdat)
library(naniar)
library(Hmisc)
library(earth)
library(ROCit)
library( pROC )
library( ggplot2 )
library(randomForest)
library(xgboost)
```

```{r}
insurance_t <- read.csv("insurance_t.csv")
insurance_v <- read.csv("insurance_v.csv")

```

```{r}
#List categoricals
categorical <- c("DDA", "DIRDEP", "NSF", "SAV", "ATM", "CD", "IRA", "INV", "MM", "MMCRED",
                 "CC", "CCPURC", "SDB", "INAREA", "INS")

#missing values
colSums(is.na(insurance_t))


#Imputed flag for missings only for non categorical variables
for (i in 1:ncol(insurance_t)) {  
  if (!(colnames(insurance_t[i]) %in% categorical)){
    col_name <- names(insurance_t)[i]
    imputation_flag_col <- ifelse(is.na(insurance_t[, i]), 1, 0)
    insurance_t <- cbind(insurance_t, imputation_flag_col)
    colnames(insurance_t)[ncol(insurance_t)] <- paste0("Missing_",col_name)}
}

colnames(insurance_t)

#Impute median for continuous
for(i in names(insurance_t)){
  if (is.numeric(insurance_t[[i]])){
    insurance_t[[i]][is.na(insurance_t[[i]])] <- median(insurance_t[[i]], na.rm = TRUE)
  }
}


#Checking for number of levels over 10 to treat as
for (i in names(insurance_t)){
  if (i %in% categorical){
    print(i)
    print(nlevels(as.factor(insurance_t[[i]])))
  }
}

#Categoricals to factors 
for (i in names(insurance_t)){
  if (i %in% categorical){
    insurance_t[[i]]<- as.factor(insurance_t[[i]])
  }
}
```

```{r}
#List categoricals
categorical <- c("DDA", "DIRDEP", "NSF", "SAV", "ATM", "CD", "IRA", "INV", "MM", "MMCRED",
                 "CC", "CCPURC", "SDB", "INAREA", "INS")

#missing values
colSums(is.na(insurance_v))


#Imputed flag for missings only for non categorical variables
for (i in 1:ncol(insurance_v)) {  
  if (!(colnames(insurance_v[i]) %in% categorical)){
    col_name <- names(insurance_v)[i]
    imputation_flag_col <- ifelse(is.na(insurance_v[, i]), 1, 0)
    insurance_v <- cbind(insurance_v, imputation_flag_col)
    colnames(insurance_v)[ncol(insurance_v)] <- paste0("Missing_",col_name)}
}

colnames(insurance_v)

#Impute median for continuous
for(i in names(insurance_v)){
  if (is.numeric(insurance_v[[i]])){
    insurance_v[[i]][is.na(insurance_v[[i]])] <- median(insurance_v[[i]], na.rm = TRUE)
  }
}


#Checking for number of levels over 10 to treat as
for (i in names(insurance_v)){
  if (i %in% categorical){
    print(i)
    print(nlevels(as.factor(insurance_v[[i]])))
  }
}

#Categoricals to factors 
for (i in names(insurance_v)){
  if (i %in% categorical){
    insurance_v[[i]]<- as.factor(insurance_v[[i]])
  }
}
```
Naive Bayes Model
```{r}
library(e1071)

set.seed(47)
nb.insurance <- naiveBayes(INS ~ ., data = insurance_t, laplace = 0, usekernel = TRUE)
summary(nb.insurance)
```
```{r}
library(caret)
library(klaR)

tune_grid <- expand.grid(
  usekernel = c(TRUE, FALSE),
  fL = c(0, 0.5, 1),
  adjust = c(0.1, 0.5, 1)
)

set.seed(47)
nb.insurance.caret <- caret::train(INS ~ ., data = insurance_t,
                       method = "nb", 
                       tuneGrid = tune_grid,
                       trControl = trainControl(method = 'cv', number = 10))

nb.insurance.caret$bestTune
```
The best NaÃ¯ve Bayes algorithm has a Laplace correction value of 0, a bandwidth adjustment of 0.1, and uses the kernel distributions for continuous predictor variables.

ROC curve
```{r}
set.seed(47)
insurance_t$p_hat <- predict(nb.insurance,insurance_t, type = 'raw')[,2]
```

```{r}
nb.roc <- roc(as.numeric(insurance_t$INS), as.numeric(insurance_t$p_hat))
plot(nb.roc, col = "pink", main = "ROC Curve for Neural Network Model")
```

```{r}
library(pROC)
auc(insurance_t$INS, insurance_t$p_hat)
```

The model with the best AUC is the XGBoost Model with AUC of .861

XGBoost
```{r}
set.seed(47)
insurance_t$INS <- as.numeric(insurance_t$INS) - 1

train_x <- model.matrix(INS ~ ., data = insurance_t)[, -1]
train_y <- insurance_t$INS

xgb.insurance <- xgboost(subsample = 1, nrounds = 15, max_depth = 5, eta = .3, nfold = 10, objective = "binary:logistic", data = train_x, label = train_y)
```
```{r}

train_y <- as.factor(insurance_t$INS)
#Tuning through caret
tune_grid <- expand.grid(
  nrounds = 10,
  eta = .25,
  max_depth = 4,
  gamma = c(0),
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb.ins.caret <- caret::train(x = train_x, y = train_y,
      method = "xgbTree",
      tuneGrid = tune_grid,
      trControl = trainControl(method = 'cv', # Using 10-fold cross-validation
                               number = 10))
```


```{r}
#Validation
set.seed(47)
insurance_v$INS <- as.numeric(insurance_v$INS) - 1

val_x <- model.matrix(INS ~ ., data = insurance_v)[, -1]
val_y <- insurance_v$INS

xgb.insurance.val <- xgboost(subsample = 1, nrounds = 15, max_depth = 5, eta = .3, nfold = 10, objective = "binary:logistic", data = val_x, label = val_y)


insurance_v$p_hat_xg <- predict(xgb.insurance.val, newdata = val_x, type = "response")


xgb_roc <- rocit(as.numeric(insurance_v$p_hat_xg), as.numeric(insurance_v$INS))
plot(xgb_roc, col = "pink")
lines(c(0, 1), c(0, 1), col = "black", lty = 2)

ciAUC(xgb_roc, level = 0.99)
summary(xgb_roc)
```

AUC of .926

#Model Interpretation
```{r}
xgb_pred <- Predictor$new(xgb.ins.caret, data = data.frame(train_x) , y = train_y)

ale_plot <- FeatureEffects$new(xgb_pred, method = "ale")
ale_plot$plot(c("ACCTAGE"))
ale_plot$plot()
```

```{r}
pd_plot <- FeatureEffects$new(xgb_pred, method = "pdp")
pd_plot$plot(c("ACCTAGE"))
pd_plot$plot()

```







